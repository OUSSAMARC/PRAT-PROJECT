{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84bc37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/Etu3/21402333/.local/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Import Libraries\n",
    "# ===========================\n",
    "\n",
    "# Deep Learning & Torch Utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Computer Vision & Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Dataset & Transformations\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Segmentation Models\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b0c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        for i in range(mask.shape[0]):  # Parcourir les lignes\n",
    "            for j in range(mask.shape[1]):  # Parcourir les colonnes\n",
    "                if 127 < mask[i, j] :  # Si la valeur du pixel est 255\n",
    "                    mask[i, j] = 1  # Remplace par 1\n",
    "                else:\n",
    "                    mask[i, j] = 0  # Remplace par 0\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c909b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    train_ds = CustomDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CustomDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def check_accuracy(loader, model, threshold=0.5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Évalue la précision et calcule le Dice Coefficient pour le modèle.\n",
    "    \n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader pour les données de validation/test.\n",
    "        model (torch.nn.Module): Modèle à évaluer.\n",
    "        threshold (float): Seuil pour binariser les prédictions.\n",
    "        device (str): Appareil cible (\"cuda\" ou \"cpu\").\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    dice_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Convertir les tenseurs en FloatTensor et déplacer sur le GPU\n",
    "            x = x.float().to(device)\n",
    "            y = y.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # Prédictions\n",
    "            preds, _ = model(x)\n",
    "\n",
    "            # Calcul du Dice Coefficient\n",
    "            preds_binarized = (torch.sigmoid(preds) > threshold).float()\n",
    "            intersection = (preds_binarized * y).sum()\n",
    "            union = preds_binarized.sum() + y.sum()\n",
    "            dice = (2.0 * intersection) / (union + 1e-6)  # Ajout d'un epsilon pour éviter la division par zéro\n",
    "            dice_scores.append(dice.item())\n",
    "\n",
    "            # Binarisation des prédictions pour la précision\n",
    "            num_correct += (preds_binarized == y).sum()\n",
    "            num_samples += preds.numel()\n",
    "\n",
    "    # Moyenne des scores de Dice\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "    acc = num_correct / num_samples\n",
    "\n",
    "    print(f\"Accuracy: {acc * 100:.2f}%, Average Dice: {avg_dice:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"saved_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15838012",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"./EBHI-SEG-Segmentation-train/image/\"\n",
    "TRAIN_MASK_DIR = \"./EBHI-SEG-Segmentation-train/label/\"\n",
    "VAL_IMG_DIR = \"./EBHI-SEG-Segmentation-test-for-eval/image/\"\n",
    "VAL_MASK_DIR = \"./EBHI-SEG-Segmentation-test-for-eval/label/\"\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.ElasticTransform(alpha=1.0, sigma=50, p=0.25),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.25),\n",
    "    A.Rotate(limit=35, p=0.7),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.15),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "        [   A.Resize(256, 256),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e92883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2058794/168585376.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=device)  # Charger les poids sauvegardés\n"
     ]
    }
   ],
   "source": [
    "# Spécifier le GPU cible\n",
    "# Spécifier le GPU cible\n",
    "device = torch.device(\"cuda:3\")\n",
    "Resnet34 = torchvision.models.resnet34(pretrained=False)\n",
    "weights_path = \"resnet34_cerberus_torchvision .pth\" \n",
    "\n",
    "# Charger les poids dans l'encodeur du modèle U-Net\n",
    "state_dict = torch.load(weights_path, map_location=device)  # Charger les poids sauvegardés\n",
    "Resnet34.load_state_dict(state_dict)  # Charger les poids dans l'encodeur\n",
    "# Déplacer le modèle complet vers le GPU cible\n",
    "Resnet34 = Resnet34.to(device)\n",
    "# Afficher un résumé du modèle\n",
    "print(Resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "212bf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, conv_in_channels, conv_out_channels, up_in_channels=None, up_out_channels=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        eg:\n",
    "        decoder1:\n",
    "        up_in_channels      : 1024,     up_out_channels     : 512\n",
    "        conv_in_channels    : 1024,     conv_out_channels   : 512\n",
    "\n",
    "        decoder5:\n",
    "        up_in_channels      : 64,       up_out_channels     : 64\n",
    "        conv_in_channels    : 128,      conv_out_channels   : 64\n",
    "        \"\"\"\n",
    "        if up_in_channels==None:\n",
    "            up_in_channels=conv_in_channels\n",
    "        if up_out_channels==None:\n",
    "            up_out_channels=conv_out_channels\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(up_in_channels, up_out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(conv_in_channels, conv_out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(conv_out_channels, conv_out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(conv_out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    # x1-upconv , x2-downconv\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UnetResnet34(nn.Module):\n",
    "    def __init__(self, num_classes=1, resnet34 = Resnet34, num_classes_classification = 6):\n",
    "        super().__init__()\n",
    "        filters = [64, 128, 256, 512]\n",
    "\n",
    "        self.firstlayer = nn.Sequential(*list(resnet34.children())[:3])\n",
    "        self.maxpool = list(resnet34.children())[3]\n",
    "        self.encoder1 = resnet34.layer1\n",
    "        self.encoder2 = resnet34.layer2\n",
    "        self.encoder3 = resnet34.layer3\n",
    "        self.encoder4 = resnet34.layer4\n",
    "\n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.Conv2d(filters[3], filters[3]*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(filters[3]*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        self.decoder1 = DecoderBlock(conv_in_channels=filters[3]*2, conv_out_channels=filters[3])\n",
    "        self.decoder2 = DecoderBlock(conv_in_channels=filters[3], conv_out_channels=filters[2])\n",
    "        self.decoder3 = DecoderBlock(conv_in_channels=filters[2], conv_out_channels=filters[1])\n",
    "        self.decoder4 = DecoderBlock(conv_in_channels=filters[1], conv_out_channels=filters[0])\n",
    "        self.decoder5 = DecoderBlock(\n",
    "            conv_in_channels=filters[1], conv_out_channels=filters[0], up_in_channels=filters[0], up_out_channels=filters[0]\n",
    "        )\n",
    "\n",
    "        self.lastlayer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=filters[0], out_channels=filters[0], kernel_size=2, stride=2),\n",
    "            nn.Conv2d(filters[0], num_classes, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        # Nouveau décodeur pour la classification\n",
    "        self.classification_decoder = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Pooling global\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1000, num_classes_classification)  # Classes de classification\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.firstlayer(x)\n",
    "        maxe1 = self.maxpool(e1)\n",
    "        e2 = self.encoder1(maxe1)\n",
    "        e3 = self.encoder2(e2)\n",
    "        e4 = self.encoder3(e3)\n",
    "        e5 = self.encoder4(e4)\n",
    "        \n",
    "        c = self.bridge(e5)\n",
    "        \n",
    "        d1 = self.decoder1(c, e5)\n",
    "        d2 = self.decoder2(d1, e4)\n",
    "        d3 = self.decoder3(d2, e3)\n",
    "        d4 = self.decoder4(d3, e2)\n",
    "        d5 = self.decoder5(d4, e1)\n",
    "        \n",
    "        class_out = self.classification_decoder(e5)\n",
    "\n",
    "        out = self.lastlayer(d5)\n",
    "\n",
    "        return out, class_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c27881",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = UnetResnet34(num_classes=1, resnet34 = Resnet34, num_classes_classification = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0120d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstlayer.0.weight: requires_grad=False\n",
      "firstlayer.1.weight: requires_grad=False\n",
      "firstlayer.1.bias: requires_grad=False\n",
      "encoder1.0.conv1.weight: requires_grad=False\n",
      "encoder1.0.bn1.weight: requires_grad=False\n",
      "encoder1.0.bn1.bias: requires_grad=False\n",
      "encoder1.0.conv2.weight: requires_grad=False\n",
      "encoder1.0.bn2.weight: requires_grad=False\n",
      "encoder1.0.bn2.bias: requires_grad=False\n",
      "encoder1.1.conv1.weight: requires_grad=False\n",
      "encoder1.1.bn1.weight: requires_grad=False\n",
      "encoder1.1.bn1.bias: requires_grad=False\n",
      "encoder1.1.conv2.weight: requires_grad=False\n",
      "encoder1.1.bn2.weight: requires_grad=False\n",
      "encoder1.1.bn2.bias: requires_grad=False\n",
      "encoder1.2.conv1.weight: requires_grad=False\n",
      "encoder1.2.bn1.weight: requires_grad=False\n",
      "encoder1.2.bn1.bias: requires_grad=False\n",
      "encoder1.2.conv2.weight: requires_grad=False\n",
      "encoder1.2.bn2.weight: requires_grad=False\n",
      "encoder1.2.bn2.bias: requires_grad=False\n",
      "encoder2.0.conv1.weight: requires_grad=False\n",
      "encoder2.0.bn1.weight: requires_grad=False\n",
      "encoder2.0.bn1.bias: requires_grad=False\n",
      "encoder2.0.conv2.weight: requires_grad=False\n",
      "encoder2.0.bn2.weight: requires_grad=False\n",
      "encoder2.0.bn2.bias: requires_grad=False\n",
      "encoder2.0.downsample.0.weight: requires_grad=False\n",
      "encoder2.0.downsample.1.weight: requires_grad=False\n",
      "encoder2.0.downsample.1.bias: requires_grad=False\n",
      "encoder2.1.conv1.weight: requires_grad=False\n",
      "encoder2.1.bn1.weight: requires_grad=False\n",
      "encoder2.1.bn1.bias: requires_grad=False\n",
      "encoder2.1.conv2.weight: requires_grad=False\n",
      "encoder2.1.bn2.weight: requires_grad=False\n",
      "encoder2.1.bn2.bias: requires_grad=False\n",
      "encoder2.2.conv1.weight: requires_grad=False\n",
      "encoder2.2.bn1.weight: requires_grad=False\n",
      "encoder2.2.bn1.bias: requires_grad=False\n",
      "encoder2.2.conv2.weight: requires_grad=False\n",
      "encoder2.2.bn2.weight: requires_grad=False\n",
      "encoder2.2.bn2.bias: requires_grad=False\n",
      "encoder2.3.conv1.weight: requires_grad=False\n",
      "encoder2.3.bn1.weight: requires_grad=False\n",
      "encoder2.3.bn1.bias: requires_grad=False\n",
      "encoder2.3.conv2.weight: requires_grad=False\n",
      "encoder2.3.bn2.weight: requires_grad=False\n",
      "encoder2.3.bn2.bias: requires_grad=False\n",
      "encoder3.0.conv1.weight: requires_grad=False\n",
      "encoder3.0.bn1.weight: requires_grad=False\n",
      "encoder3.0.bn1.bias: requires_grad=False\n",
      "encoder3.0.conv2.weight: requires_grad=False\n",
      "encoder3.0.bn2.weight: requires_grad=False\n",
      "encoder3.0.bn2.bias: requires_grad=False\n",
      "encoder3.0.downsample.0.weight: requires_grad=False\n",
      "encoder3.0.downsample.1.weight: requires_grad=False\n",
      "encoder3.0.downsample.1.bias: requires_grad=False\n",
      "encoder3.1.conv1.weight: requires_grad=False\n",
      "encoder3.1.bn1.weight: requires_grad=False\n",
      "encoder3.1.bn1.bias: requires_grad=False\n",
      "encoder3.1.conv2.weight: requires_grad=False\n",
      "encoder3.1.bn2.weight: requires_grad=False\n",
      "encoder3.1.bn2.bias: requires_grad=False\n",
      "encoder3.2.conv1.weight: requires_grad=False\n",
      "encoder3.2.bn1.weight: requires_grad=False\n",
      "encoder3.2.bn1.bias: requires_grad=False\n",
      "encoder3.2.conv2.weight: requires_grad=False\n",
      "encoder3.2.bn2.weight: requires_grad=False\n",
      "encoder3.2.bn2.bias: requires_grad=False\n",
      "encoder3.3.conv1.weight: requires_grad=False\n",
      "encoder3.3.bn1.weight: requires_grad=False\n",
      "encoder3.3.bn1.bias: requires_grad=False\n",
      "encoder3.3.conv2.weight: requires_grad=False\n",
      "encoder3.3.bn2.weight: requires_grad=False\n",
      "encoder3.3.bn2.bias: requires_grad=False\n",
      "encoder3.4.conv1.weight: requires_grad=False\n",
      "encoder3.4.bn1.weight: requires_grad=False\n",
      "encoder3.4.bn1.bias: requires_grad=False\n",
      "encoder3.4.conv2.weight: requires_grad=False\n",
      "encoder3.4.bn2.weight: requires_grad=False\n",
      "encoder3.4.bn2.bias: requires_grad=False\n",
      "encoder3.5.conv1.weight: requires_grad=False\n",
      "encoder3.5.bn1.weight: requires_grad=False\n",
      "encoder3.5.bn1.bias: requires_grad=False\n",
      "encoder3.5.conv2.weight: requires_grad=False\n",
      "encoder3.5.bn2.weight: requires_grad=False\n",
      "encoder3.5.bn2.bias: requires_grad=False\n",
      "encoder4.0.conv1.weight: requires_grad=False\n",
      "encoder4.0.bn1.weight: requires_grad=False\n",
      "encoder4.0.bn1.bias: requires_grad=False\n",
      "encoder4.0.conv2.weight: requires_grad=False\n",
      "encoder4.0.bn2.weight: requires_grad=False\n",
      "encoder4.0.bn2.bias: requires_grad=False\n",
      "encoder4.0.downsample.0.weight: requires_grad=False\n",
      "encoder4.0.downsample.1.weight: requires_grad=False\n",
      "encoder4.0.downsample.1.bias: requires_grad=False\n",
      "encoder4.1.conv1.weight: requires_grad=False\n",
      "encoder4.1.bn1.weight: requires_grad=False\n",
      "encoder4.1.bn1.bias: requires_grad=False\n",
      "encoder4.1.conv2.weight: requires_grad=False\n",
      "encoder4.1.bn2.weight: requires_grad=False\n",
      "encoder4.1.bn2.bias: requires_grad=False\n",
      "encoder4.2.conv1.weight: requires_grad=False\n",
      "encoder4.2.bn1.weight: requires_grad=False\n",
      "encoder4.2.bn1.bias: requires_grad=False\n",
      "encoder4.2.conv2.weight: requires_grad=False\n",
      "encoder4.2.bn2.weight: requires_grad=False\n",
      "encoder4.2.bn2.bias: requires_grad=False\n",
      "bridge.0.weight: requires_grad=True\n",
      "bridge.1.weight: requires_grad=True\n",
      "bridge.1.bias: requires_grad=True\n",
      "decoder1.up.weight: requires_grad=True\n",
      "decoder1.up.bias: requires_grad=True\n",
      "decoder1.conv.0.weight: requires_grad=True\n",
      "decoder1.conv.1.weight: requires_grad=True\n",
      "decoder1.conv.1.bias: requires_grad=True\n",
      "decoder1.conv.3.weight: requires_grad=True\n",
      "decoder1.conv.4.weight: requires_grad=True\n",
      "decoder1.conv.4.bias: requires_grad=True\n",
      "decoder2.up.weight: requires_grad=True\n",
      "decoder2.up.bias: requires_grad=True\n",
      "decoder2.conv.0.weight: requires_grad=True\n",
      "decoder2.conv.1.weight: requires_grad=True\n",
      "decoder2.conv.1.bias: requires_grad=True\n",
      "decoder2.conv.3.weight: requires_grad=True\n",
      "decoder2.conv.4.weight: requires_grad=True\n",
      "decoder2.conv.4.bias: requires_grad=True\n",
      "decoder3.up.weight: requires_grad=True\n",
      "decoder3.up.bias: requires_grad=True\n",
      "decoder3.conv.0.weight: requires_grad=True\n",
      "decoder3.conv.1.weight: requires_grad=True\n",
      "decoder3.conv.1.bias: requires_grad=True\n",
      "decoder3.conv.3.weight: requires_grad=True\n",
      "decoder3.conv.4.weight: requires_grad=True\n",
      "decoder3.conv.4.bias: requires_grad=True\n",
      "decoder4.up.weight: requires_grad=True\n",
      "decoder4.up.bias: requires_grad=True\n",
      "decoder4.conv.0.weight: requires_grad=True\n",
      "decoder4.conv.1.weight: requires_grad=True\n",
      "decoder4.conv.1.bias: requires_grad=True\n",
      "decoder4.conv.3.weight: requires_grad=True\n",
      "decoder4.conv.4.weight: requires_grad=True\n",
      "decoder4.conv.4.bias: requires_grad=True\n",
      "decoder5.up.weight: requires_grad=True\n",
      "decoder5.up.bias: requires_grad=True\n",
      "decoder5.conv.0.weight: requires_grad=True\n",
      "decoder5.conv.1.weight: requires_grad=True\n",
      "decoder5.conv.1.bias: requires_grad=True\n",
      "decoder5.conv.3.weight: requires_grad=True\n",
      "decoder5.conv.4.weight: requires_grad=True\n",
      "decoder5.conv.4.bias: requires_grad=True\n",
      "lastlayer.0.weight: requires_grad=True\n",
      "lastlayer.0.bias: requires_grad=True\n",
      "lastlayer.1.weight: requires_grad=True\n",
      "classification_decoder.2.weight: requires_grad=False\n",
      "classification_decoder.2.bias: requires_grad=False\n",
      "classification_decoder.4.weight: requires_grad=False\n",
      "classification_decoder.4.bias: requires_grad=False\n"
     ]
    }
   ],
   "source": [
    "# Geler les poids de l'encodeur (ResNet34)\n",
    "for param in new_model.parameters() :\n",
    "    param.requires_grad = False\n",
    "# for param in new_model.classification_decoder.parameters() :\n",
    "    #param.requires_grad = True\n",
    "\n",
    "for param in new_model.bridge.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.decoder1.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.decoder2.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.decoder3.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.decoder4.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.decoder5.parameters() :\n",
    "    param.requires_grad = True\n",
    "for param in new_model.lastlayer.parameters() :\n",
    "    param.requires_grad = True\n",
    "    \n",
    "    \n",
    "for name, param in new_model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af94ad",
   "metadata": {},
   "source": [
    "# Avec de la data augmentation et scheduele lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6da776ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2058794/474051137.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Epoch [1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                               | 0/56 [00:00<?, ?it/s]/tmp/ipykernel_2058794/474051137.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:20<00:00,  2.76it/s, loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3410, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.16%, Average Dice: 0.9176\n",
      "Epoch [2/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1896, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.68%, Average Dice: 0.9340\n",
      "Epoch [3/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2621, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.20%, Average Dice: 0.9364\n",
      "Epoch [4/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1716, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.85%, Average Dice: 0.9423\n",
      "Epoch [5/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.89it/s, loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1837, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.75%, Average Dice: 0.9419\n",
      "Epoch [6/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3358, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.55%, Average Dice: 0.9388\n",
      "Epoch [7/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2329, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.97%, Average Dice: 0.9439\n",
      "Epoch [8/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2165, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.06%, Average Dice: 0.9450\n",
      "Epoch [9/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1822, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.19%, Average Dice: 0.9453\n",
      "Epoch [10/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1708, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.32%, Average Dice: 0.9463\n",
      "Epoch [11/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1239, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.99%, Average Dice: 0.9427\n",
      "Epoch [12/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1905, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.54%, Average Dice: 0.9383\n",
      "Epoch [13/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1980, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.22%, Average Dice: 0.9459\n",
      "Epoch [14/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2319, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.75%, Average Dice: 0.9499\n",
      "Epoch [15/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.89it/s, loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1569, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.87%, Average Dice: 0.9442\n",
      "Epoch [16/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1392, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.60%, Average Dice: 0.9489\n",
      "Epoch [17/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.89it/s, loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1625, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.66%, Average Dice: 0.9492\n",
      "Epoch [18/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.239]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2395, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.65%, Average Dice: 0.9488\n",
      "Epoch [19/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2075, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.59%, Average Dice: 0.9483\n",
      "Epoch [20/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.89it/s, loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1527, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.62%, Average Dice: 0.9485\n",
      "Epoch [21/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1911, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.65%, Average Dice: 0.9489\n",
      "Epoch [22/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1183, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.72%, Average Dice: 0.9497\n",
      "Epoch [23/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1364, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.60%, Average Dice: 0.9484\n",
      "Epoch [24/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1278, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.72%, Average Dice: 0.9495\n",
      "Epoch [25/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1396, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.72%, Average Dice: 0.9497\n",
      "Epoch [26/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.91it/s, loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1512, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.65%, Average Dice: 0.9489\n",
      "Epoch [27/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1517, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.57%, Average Dice: 0.9480\n",
      "Epoch [28/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1936, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.72%, Average Dice: 0.9497\n",
      "Epoch [29/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.90it/s, loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1610, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.68%, Average Dice: 0.9493\n",
      "Epoch [30/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████| 56/56 [00:19<00:00,  2.87it/s, loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1961, device='cuda:3',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.77%, Average Dice: 0.9502\n"
     ]
    }
   ],
   "source": [
    "# Passer le modèle sur le GPU\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model=new_model.to(device)\n",
    "\n",
    "# Définir les hyperparamètres\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Fonction de perte pour segmentation binaire\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "# Initialiser le scaler pour le calcul mixte (amp)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}]\")\n",
    "    model.train()\n",
    "    train_loader, _ = get_loaders(\n",
    "        TRAIN_IMG_DIR,\n",
    "        TRAIN_MASK_DIR,\n",
    "        VAL_IMG_DIR,\n",
    "        VAL_MASK_DIR,\n",
    "        BATCH_SIZE,\n",
    "        train_transform,\n",
    "        val_transforms,\n",
    "        NUM_WORKERS,\n",
    "        PIN_MEMORY,\n",
    "    )\n",
    "    loop = tqdm(train_loader, total=len(train_loader), desc=\"Training\")\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        # Charger les données sur GPU\n",
    "        data = data.float().to(device)\n",
    "        targets = targets.float().unsqueeze(1).to(device)  # Ajouter une dimension pour le canal de sortie\n",
    "\n",
    "        # Forward pass avec AMP (Mixed Precision)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions, _ = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Mise à jour de la barre de progression\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    scheduler.step()     \n",
    "    print(loss)\n",
    "    # Évaluer la précision après chaque époque\n",
    "    check_accuracy(val_loader, model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
